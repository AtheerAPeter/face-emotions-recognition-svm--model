{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Idea: Real-Time Client Satisfaction Detection System for Service Industries\n",
    "\n",
    "Overview: \n",
    "Develop a \"Real-Time Client Satisfaction Detection System\" specifically designed for service industries such as hospitality, banking, healthcare, and customer support centers. This system utilizes face emotion detection technology to gauge client satisfaction in real time, providing immediate feedback to service providers to enhance the client experience dynamically.\n",
    "\n",
    "Problem statement:\n",
    "Not know user satisfaction for a particular service or product.\n",
    "\n",
    "Why is it important:\n",
    "This provides real time feedback of user opinion.\n",
    "\n",
    "How solving this problem will benefit the company:\n",
    "By logging user emotions when accessing let's say a certain screen in an app, we can have an idea if this screen is user friendly for example if the user seems to give positive emotions or it is bad and needs to be fixed if user face emotions are negarive. This provides a faster way of feedback that the user does not need to do anything about other than give consent to be read in certain places in the software.\n",
    "\n",
    "Another business idea would be to replace emojis with face emotions in chatting apps, for example send someone your mood which is read by this model instead of an emoji. I have implemented a similar idea about 4 years ago in this project (https://retrochat97.netlify.app/) \n",
    "\n",
    "How would you collect relevant data:\n",
    "Data was taken from kaggle (https://www.kaggle.com/datasets/msambare/fer2013)\n",
    "\n",
    "How would you formulate this problem as a machine learning task:\n",
    "The data from the source above comes in two folders, training and testing, each containing sub folders which are named after the emotion of the images inside them, angry, happy, sad, neutral, disgust, fear, and surprise. The idea i had to manage this is by combining both folders into one folder containing the sub foders of emotions with the 48x48 grey scale images inside, then read them with a loop and flatten them into grey scale in a dataframe with the assigned class for each row, now i have a dataframe which i can proceed the usual with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_number = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"sad\": 4,\n",
    "    \"surprise\": 5,\n",
    "    \"neutral\": 6\n",
    "}\n",
    "folder = \"./db\"\n",
    "data = []\n",
    "for label in os.listdir(folder):\n",
    "    label_folder = os.path.join(folder, label)\n",
    "    if os.path.isdir(label_folder): \n",
    "        for filename in os.listdir(label_folder):\n",
    "            img_path = os.path.join(label_folder, filename)\n",
    "            img = Image.open(img_path).convert('L') \n",
    "            img = img.resize((48, 48))\n",
    "            img = np.array(img)\n",
    "            img = img.flatten()\n",
    "            data.append(np.append(img, emotion_to_number[label.lower()])) \n",
    "# shuffle to get them mixed up for fair split \n",
    "data = shuffle(data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration:\n",
    "\n",
    "The dataframe is made from a folder contianing sub folders with images inside them so i already know there are no missing values.\n",
    "\n",
    "Good to show that the data shape is 35887 records with 2304 columns which is exactly 48x48, a flattened grey scale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253</td>\n",
       "      <td>254</td>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "      <td>245</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>169</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>254</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>254</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196</td>\n",
       "      <td>223</td>\n",
       "      <td>136</td>\n",
       "      <td>163</td>\n",
       "      <td>127</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "      <td>106</td>\n",
       "      <td>96</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>219</td>\n",
       "      <td>205</td>\n",
       "      <td>200</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>162</td>\n",
       "      <td>156</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233</td>\n",
       "      <td>216</td>\n",
       "      <td>179</td>\n",
       "      <td>157</td>\n",
       "      <td>144</td>\n",
       "      <td>145</td>\n",
       "      <td>155</td>\n",
       "      <td>171</td>\n",
       "      <td>183</td>\n",
       "      <td>177</td>\n",
       "      <td>...</td>\n",
       "      <td>222</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>83</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>153</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>204</td>\n",
       "      <td>224</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>224</td>\n",
       "      <td>225</td>\n",
       "      <td>226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  2295  2296  2297  \\\n",
       "0  253  254  255  253  255  245  208  172  169  151  ...   253   254   255   \n",
       "1  196  223  136  163  127  140  128  106   96  102  ...   219   205   200   \n",
       "2  233  216  179  157  144  145  155  171  183  177  ...   222   221   221   \n",
       "3    6    6    9    9   83  114   35   18   28   20  ...   153    78    17   \n",
       "4  185   50    3   19    9    9    7   12   26    8  ...   111   204   224   \n",
       "\n",
       "   2298  2299  2300  2301  2302  2303  emotion  \n",
       "0   255   254   255   255   253   255        2  \n",
       "1   197   196   191   190   162   156        3  \n",
       "2   221   221   221   221   221   221        2  \n",
       "3     8     9     9    11     6     8        4  \n",
       "4   226   225   225   224   225   226        3  \n",
       "\n",
       "[5 rows x 2305 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=[*range(2304), \"emotion\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 2304)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['emotion'], axis=1)\n",
    "y = df['emotion']\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304) (7178, 2304) (28709,) (7178,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature engineering\n",
    "\n",
    "Scaling the training data with StandardScaler to avoid ranking them based on grey scale values which is not desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304)\n",
      "(7178, 2304)\n",
      "[[ 0.13290277  0.43354239  0.42639928 ...  0.0037627   0.08315867\n",
      "   0.1848582 ]\n",
      " [ 1.43233363  0.19849304 -0.11400979 ... -1.17533395 -1.26578681\n",
      "  -1.22774702]\n",
      " [ 1.62664105  1.67064426  1.46951447 ...  1.83071466  1.81751714\n",
      "   1.76290368]\n",
      " ...\n",
      " [-0.58360584 -1.00149578 -0.98117783 ...  1.84367176  1.81751714\n",
      "   1.77562985]\n",
      " [ 0.74011345  0.79230193  0.84113183 ... -0.09989415 -0.58489052\n",
      "  -0.97322356]\n",
      " [-1.44584501 -1.44685245 -1.42104568 ... -0.38495048 -0.54634922\n",
      "  -1.07503294]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing PCA to reduce the number of features from 2304 to 274, retaining 95% of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 274)\n",
      "(7178, 274)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different models, SVM with rbf kernel turned out to give the best accuracy here so it has been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=2, max_iter=100000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=2, max_iter=100000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=2, max_iter=100000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_rbf_model = SVC(kernel='rbf', max_iter=100000, C=2)\n",
    "svc_rbf_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svc_rbf_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4743661186960156"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model assessment step\n",
    "\n",
    "The model is not ideal for this task but since this is what we have taken in this semster, so be it for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the model, scaler, and pca to joblib to be used in front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(svc_rbf_model, 'model.joblib')\n",
    "dump(pca, 'pca.joblib')\n",
    "dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model in a real environment:\n",
    "\n",
    "By integrating the joblib model to a next js project using react, can be found in this repo (https://github.com/AtheerAPeter/face-emotions-recognition-svm)\n",
    "\n",
    "The implementation is made using Next js,\n",
    "after cloning the repo, node modules need to be installed by `npm i` or `yarn` then run `npm run dev` to run it on `http://localhost:3000`\n",
    "this link shows a camera open with a button (predict) which predicts the emotion of the face shown in the camera view.\n",
    "\n",
    "it also has a service which can be used in other applications by sending the image as a base64 string to the end point `http://localhost:3000/predict`\n",
    "\n",
    "**The end point can be hit with**\n",
    "```\n",
    "curl --location 'http://localhost:3000/api/predict' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "\n",
    "    \"image\":\"data:image/jpeg;base64,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\"\n",
    "}'\n",
    "```\n",
    "\n",
    "this is a cURL request with a base64 image that shows a happy face the response should be \"3\"\n",
    "\n",
    "response:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prediction\": \"3\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "As expected it returns \"3\" which means happy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "\n",
    "The end point takes the base64 image string, saves it to tmp folder then calls the python script which reads the image from tmp folder, resizes it to 48*48, converts it to grey scale, flattens it, runs it by the scaler and the pca to have a similar looking set of features as the model is trained on, then it runs it by the model, then the prediction is saved in a csv in the tmp folder then its back to the controller in the end point which will read the csv file and sends the result of it as a respoonse to the frontend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
